{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet, Imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet3D, ElucidatedImagen, ImagenTrainer\n",
    "\n",
    "# unet1 = Unet3D(dim = 64, dim_mults = (1, 2, 4, 8)).cuda()\n",
    "# unet2 = Unet3D(dim = 64, dim_mults = (1, 2, 4, 8)).cuda()\n",
    "\n",
    "# elucidated imagen, which contains the unets above (base unet and super resoluting ones)\n",
    "# imagen = ElucidatedImagen(\n",
    "#     unets = (unet1, unet2),\n",
    "#     image_sizes = (16, 32),\n",
    "#     random_crop_sizes = (None, 16),\n",
    "#     temporal_downsample_factor = (2, 1),        # in this example, the first unet would receive the video temporally downsampled by 2x\n",
    "#     num_sample_steps = 10,\n",
    "#     cond_drop_prob = 0.1,\n",
    "#     sigma_min = 0.002,                          # min noise level\n",
    "#     sigma_max = (80, 160),                      # max noise level, double the max noise level for upsampler\n",
    "#     sigma_data = 0.5,                           # standard deviation of data distribution\n",
    "#     rho = 7,                                    # controls the sampling schedule\n",
    "#     P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n",
    "#     P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n",
    "#     S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n",
    "#     S_tmin = 0.05,\n",
    "#     S_tmax = 50,\n",
    "#     S_noise = 1.003,\n",
    "# ).cuda()\n",
    "\n",
    "# # mock videos (get a lot of this) and text encodings from large T5\n",
    "\n",
    "# texts = [\n",
    "#     'a whale breaching from afar',\n",
    "#     'young girl blowing out candles on her birthday cake',\n",
    "#     'fireworks with blue and green sparkles',\n",
    "#     'dust motes swirling in the morning sunshine on the windowsill'\n",
    "# ]\n",
    "\n",
    "# videos = torch.randn(4, 3, 10, 32, 32).cuda() # (batch, channels, time / video frames, height, width)\n",
    "\n",
    "# # feed images into imagen, training each unet in the cascade\n",
    "# # for this example, only training unet 1\n",
    "\n",
    "# trainer = ImagenTrainer(imagen)\n",
    "\n",
    "# # you can also ignore time when training on video initially, shown to improve results in video-ddpm paper. eventually will make the 3d unet trainable with either images or video. research shows it is essential (with current data regimes) to train first on text-to-image. probably won't be true in another decade. all big data becomes small data\n",
    "\n",
    "# trainer(videos, texts = texts, unet_number = 1, ignore_time = False)\n",
    "# trainer.update(unet_number = 1)\n",
    "\n",
    "# videos = trainer.sample(texts = texts, video_frames = 20) # extrapolating to 20 frames from training on 10 frames\n",
    "# videos.shape # (4, 3, 20, 32, 32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
